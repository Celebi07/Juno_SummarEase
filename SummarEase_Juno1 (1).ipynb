{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTmMCTsMwaHv",
        "outputId": "74abcdcd-9a74-4d1e-ede6-cf0fe08955a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-telegram-bot\n",
            "  Downloading python_telegram_bot-20.2-py3-none-any.whl (535 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m535.8/535.8 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx~=0.23.3\n",
            "  Downloading httpx-0.23.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986[idna2008]<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from httpx~=0.23.3->python-telegram-bot) (2022.12.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.9/dist-packages (from httpx~=0.23.3->python-telegram-bot) (1.3.0)\n",
            "Collecting httpcore<0.17.0,>=0.15.0\n",
            "  Downloading httpcore-0.16.3-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.6/69.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.9/dist-packages (from httpcore<0.17.0,>=0.15.0->httpx~=0.23.3->python-telegram-bot) (3.6.2)\n",
            "Collecting h11<0.15,>=0.13\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.9/dist-packages (from rfc3986[idna2008]<2,>=1.3->httpx~=0.23.3->python-telegram-bot) (3.4)\n",
            "Installing collected packages: rfc3986, h11, httpcore, httpx, python-telegram-bot\n",
            "Successfully installed h11-0.14.0 httpcore-0.16.3 httpx-0.23.3 python-telegram-bot-20.2 rfc3986-1.5.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting youtube_transcript_api\n",
            "  Downloading youtube_transcript_api-0.6.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from youtube_transcript_api) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->youtube_transcript_api) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->youtube_transcript_api) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->youtube_transcript_api) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->youtube_transcript_api) (2022.12.7)\n",
            "Installing collected packages: youtube_transcript_api\n",
            "Successfully installed youtube_transcript_api-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.4-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.27.1)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.3/269.3 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.4 yarl-1.9.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: python-telegram-bot in /usr/local/lib/python3.9/dist-packages (20.2)\n",
            "Requirement already satisfied: httpx~=0.23.3 in /usr/local/lib/python3.9/dist-packages (from python-telegram-bot) (0.23.3)\n",
            "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /usr/local/lib/python3.9/dist-packages (from httpx~=0.23.3->python-telegram-bot) (1.5.0)\n",
            "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in /usr/local/lib/python3.9/dist-packages (from httpx~=0.23.3->python-telegram-bot) (0.16.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from httpx~=0.23.3->python-telegram-bot) (2022.12.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.9/dist-packages (from httpx~=0.23.3->python-telegram-bot) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.9/dist-packages (from httpcore<0.17.0,>=0.15.0->httpx~=0.23.3->python-telegram-bot) (0.14.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.9/dist-packages (from httpcore<0.17.0,>=0.15.0->httpx~=0.23.3->python-telegram-bot) (3.6.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.9/dist-packages (from rfc3986[idna2008]<2,>=1.3->httpx~=0.23.3->python-telegram-bot) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-telegram-bot==13.7\n",
            "  Downloading python_telegram_bot-13.7-py3-none-any.whl (490 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m490.1/490.1 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cachetools==4.2.2\n",
            "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from python-telegram-bot==13.7) (2022.12.7)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.9/dist-packages (from python-telegram-bot==13.7) (6.2)\n",
            "Collecting APScheduler==3.6.3\n",
            "  Downloading APScheduler-3.6.3-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2018.6 in /usr/local/lib/python3.9/dist-packages (from python-telegram-bot==13.7) (2022.7.1)\n",
            "Requirement already satisfied: setuptools>=0.7 in /usr/local/lib/python3.9/dist-packages (from APScheduler==3.6.3->python-telegram-bot==13.7) (67.6.1)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from APScheduler==3.6.3->python-telegram-bot==13.7) (1.16.0)\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.9/dist-packages (from APScheduler==3.6.3->python-telegram-bot==13.7) (4.3)\n",
            "Requirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.9/dist-packages (from tzlocal>=1.2->APScheduler==3.6.3->python-telegram-bot==13.7) (0.1.0.post0)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.9/dist-packages (from pytz-deprecation-shim->tzlocal>=1.2->APScheduler==3.6.3->python-telegram-bot==13.7) (2023.3)\n",
            "Installing collected packages: cachetools, APScheduler, python-telegram-bot\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 5.3.0\n",
            "    Uninstalling cachetools-5.3.0:\n",
            "      Successfully uninstalled cachetools-5.3.0\n",
            "  Attempting uninstall: python-telegram-bot\n",
            "    Found existing installation: python-telegram-bot 20.2\n",
            "    Uninstalling python-telegram-bot-20.2:\n",
            "      Successfully uninstalled python-telegram-bot-20.2\n",
            "Successfully installed APScheduler-3.6.3 cachetools-4.2.2 python-telegram-bot-13.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyTelegramBotAPI\n",
            "  Downloading pyTelegramBotAPI-4.11.0.tar.gz (230 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.4/230.4 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from pyTelegramBotAPI) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->pyTelegramBotAPI) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->pyTelegramBotAPI) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->pyTelegramBotAPI) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->pyTelegramBotAPI) (2022.12.7)\n",
            "Building wheels for collected packages: pyTelegramBotAPI\n",
            "  Building wheel for pyTelegramBotAPI (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyTelegramBotAPI: filename=pyTelegramBotAPI-4.11.0-py3-none-any.whl size=212399 sha256=a5f362ca10f705b6c01e96cf626b5d72d6a18c32250fea76230f94b00828b7b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/00/29/5a29bbb5ebced37cb04e5d93102dd62e765f7a4e9ca3481036\n",
            "Successfully built pyTelegramBotAPI\n",
            "Installing collected packages: pyTelegramBotAPI\n",
            "Successfully installed pyTelegramBotAPI-4.11.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.28.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textract\n",
            "  Downloading textract-1.6.5-py3-none-any.whl (23 kB)\n",
            "Collecting chardet==3.*\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20191110\n",
            "  Downloading pdfminer.six-20191110-py2.py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting argcomplete~=1.10.0\n",
            "  Downloading argcomplete-1.10.3-py2.py3-none-any.whl (36 kB)\n",
            "Collecting SpeechRecognition~=3.8.1\n",
            "  Downloading SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xlrd~=1.2.0\n",
            "  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting six~=1.12.0\n",
            "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting python-pptx~=0.6.18\n",
            "  Downloading python-pptx-0.6.21.tar.gz (10.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting beautifulsoup4~=4.8.0\n",
            "  Downloading beautifulsoup4-4.8.2-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting extract-msg<=0.29.*\n",
            "  Downloading extract_msg-0.28.7-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.0/69.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docx2txt~=0.8\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycryptodome\n",
            "  Downloading pycryptodome-3.17-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.9/dist-packages (from pdfminer.six==20191110->textract) (2.4.0)\n",
            "Requirement already satisfied: soupsieve>=1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4~=4.8.0->textract) (2.4.1)\n",
            "Collecting imapclient==2.1.0\n",
            "  Downloading IMAPClient-2.1.0-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.0/74.0 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting compressed-rtf>=1.0.6\n",
            "  Downloading compressed_rtf-1.0.6.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting olefile>=0.46\n",
            "  Downloading olefile-0.46.zip (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ebcdic>=1.1.1\n",
            "  Downloading ebcdic-1.1.1-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.5/128.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tzlocal>=2.1 in /usr/local/lib/python3.9/dist-packages (from extract-msg<=0.29.*->textract) (4.3)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from python-pptx~=0.6.18->textract) (4.9.2)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.9/dist-packages (from python-pptx~=0.6.18->textract) (8.4.0)\n",
            "Collecting XlsxWriter>=0.5.7\n",
            "  Downloading XlsxWriter-3.1.0-py3-none-any.whl (152 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.7/152.7 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.9/dist-packages (from tzlocal>=2.1->extract-msg<=0.29.*->textract) (0.1.0.post0)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.9/dist-packages (from pytz-deprecation-shim->tzlocal>=2.1->extract-msg<=0.29.*->textract) (2023.3)\n",
            "Building wheels for collected packages: docx2txt, python-pptx, compressed-rtf, olefile\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3977 sha256=f2943cf32712afbf51ac4317a900a3a4a86d24261767b207df0211871aacf6df\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/75/01/e6c444034338bde9c7947d3467807f889123465c2371e77418\n",
            "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-pptx: filename=python_pptx-0.6.21-py3-none-any.whl size=470949 sha256=824fd89ea95d337b1485e7ff426288fb6eefb84618842cc0af43992071428f0f\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/4a/ed/9653bc799915f52dce3f04d14946fbd85cce9c3cdedc9cfa71\n",
            "  Building wheel for compressed-rtf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for compressed-rtf: filename=compressed_rtf-1.0.6-py3-none-any.whl size=6201 sha256=7b9265d40035d876657119168f5aa90a8cbb74010d8b64e874dadf03860a5263\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/67/e4/ba2159853bdd0fe99330aa1e384915108143a5370686ea446f\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35432 sha256=cd82dec1854f4dd014818dfce38c48cb24c05e3e4ba8bd0a1b57bcffde4d51f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/b8/ba/ebba30390fbd997074f35e42a842ce3fd933213cac8753414e\n",
            "Successfully built docx2txt python-pptx compressed-rtf olefile\n",
            "Installing collected packages: SpeechRecognition, ebcdic, docx2txt, compressed-rtf, chardet, argcomplete, XlsxWriter, xlrd, six, pycryptodome, olefile, beautifulsoup4, python-pptx, pdfminer.six, imapclient, extract-msg, textract\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 4.0.0\n",
            "    Uninstalling chardet-4.0.0:\n",
            "      Successfully uninstalled chardet-4.0.0\n",
            "  Attempting uninstall: xlrd\n",
            "    Found existing installation: xlrd 2.0.1\n",
            "    Uninstalling xlrd-2.0.1:\n",
            "      Successfully uninstalled xlrd-2.0.1\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.16.0\n",
            "    Uninstalling six-1.16.0:\n",
            "      Successfully uninstalled six-1.16.0\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.11.2\n",
            "    Uninstalling beautifulsoup4-4.11.2:\n",
            "      Successfully uninstalled beautifulsoup4-4.11.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yfinance 0.2.18 requires beautifulsoup4>=4.11.1, but you have beautifulsoup4 4.8.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed SpeechRecognition-3.8.1 XlsxWriter-3.1.0 argcomplete-1.10.3 beautifulsoup4-4.8.2 chardet-3.0.4 compressed-rtf-1.0.6 docx2txt-0.8 ebcdic-1.1.1 extract-msg-0.28.7 imapclient-2.1.0 olefile-0.46 pdfminer.six-20191110 pycryptodome-3.17 python-pptx-0.6.21 six-1.12.0 textract-1.6.5 xlrd-1.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from bs4) (4.8.2)\n",
            "Requirement already satisfied: soupsieve>=1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->bs4) (2.4.1)\n",
            "Building wheels for collected packages: bs4\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1270 sha256=7cf761a9dfdf4cb3a72e8775086b687d1317f5d3da8570c9c754d45c5d0dd744\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/2b/cb/099980278a0c9a3e57ff1a89875ec07bfa0b6fcbebb9a8cad3\n",
            "Successfully built bs4\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from langdetect) (1.12.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993243 sha256=e0cc57ea03f4e33635643a3e3589e37b906a87061a5ebcfe0a3a8ac923558567\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/c1/d9/7e068de779d863bc8f8fc9467d85e25cfe47fa5051fff1a1bb\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ],
      "source": [
        "!pip install python-telegram-bot\n",
        "!pip install youtube_transcript_api\n",
        "!pip install openai\n",
        "!pip install --upgrade python-telegram-bot\n",
        "!pip install python-telegram-bot==13.7\n",
        "!pip install pyTelegramBotAPI\n",
        "!pip install transformers\n",
        "!pip install textract\n",
        "!pip install bs4\n",
        "!pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from youtube_transcript_api.formatters import TextFormatter\n",
        "import openai\n",
        "import os\n",
        "import textwrap\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import textwrap\n",
        "import textract\n",
        "\n",
        "openai.api_key = \"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\"\n",
        "PROMPT_STRING = \"Summarize the following text:\\n<<SUMMARY>>\\n\"\n",
        "import os\n",
        "import telebot\n",
        "\n",
        "my_telegrambot_secret = \"5632868573:AAHVK_LEzgFKNgy9SHUPC2M_9yX-B1l8YDs\"\n",
        "# API_KEY = os.getenv('API_KEY')\n",
        "bot = telebot.TeleBot(my_telegrambot_secret)\n",
        "\n",
        "# Create your AI model functions here\n",
        "def sentiment(message):\n",
        "    # Define the text to analyze\n",
        "    text = message\n",
        "\n",
        "    # Define the prompt and parameters for sentiment analysis\n",
        "    sentiment_prompt = (f\"Please analyze the sentiment of the following text:\\n{text}\\nSentiment:\")\n",
        "\n",
        "    # Define the GPT-3 parameters for sentiment analysis\n",
        "    sentiment_model = \"text-davinci-002\"\n",
        "    sentiment_temperature = 0.5 \n",
        "    sentiment_max_tokens = 1\n",
        "\n",
        "    # Call the OpenAI GPT-3 API to analyze the sentiment\n",
        "    sentiment_response = openai.Completion.create(\n",
        "        engine=sentiment_model,\n",
        "        prompt=sentiment_prompt,\n",
        "        temperature=sentiment_temperature,\n",
        "        max_tokens=sentiment_max_tokens,\n",
        "    )\n",
        "\n",
        "    # Parse the response and return the sentiment\n",
        "    return sentiment_response.choices[0].text.strip()\n",
        "\n",
        "def summarize_video(video_link):\n",
        "    # Extract the video ID from the link\n",
        "    video_id = video_link.split(\"=\")[-1]\n",
        "\n",
        "    # Get transcript for given YouTube video id\n",
        "    transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "\n",
        "    # Format transcript using TextFormatter from youtube_transcript_api library\n",
        "    formatter = TextFormatter()\n",
        "    transcript = formatter.format_transcript(transcript)\n",
        "\n",
        "    video_length = len(transcript)\n",
        "\n",
        "    # If the video is ~25 minutes or more, double the chunk size\n",
        "    # This is done to reduce overall amount of API calls\n",
        "    chunk_size = 4000 if video_length >= 25000 else 2000\n",
        "\n",
        "    # Wrap the transcript in chunks of characters\n",
        "    chunks = textwrap.wrap(transcript, chunk_size)\n",
        "\n",
        "    summaries = []\n",
        "\n",
        "    # For each chunk of characters, generate a summary\n",
        "    for chunk in chunks:\n",
        "        prompt = f\"Write a detailed summary of the following:\\n\\n{chunk}\\n\"\n",
        "\n",
        "        # Generate summary using GPT-3\n",
        "        # If the davinci model is incurring too much cost, \n",
        "        # the text-curie-001 model may be used in its place.\n",
        "        response = openai.Completion.create(\n",
        "            model=\"text-davinci-003\", prompt=prompt, max_tokens=256\n",
        "        )\n",
        "        summary = re.sub(\"\\s+\", \" \", response.choices[0].text.strip())\n",
        "        summaries.append(summary)\n",
        "\n",
        "    # Join the chunk summaries into one string\n",
        "    chunk_summaries = \" \".join(summaries)\n",
        "    prompt = f\"Write a detailed summary of the following:\\n\\n{chunk_summaries}\\n\"\n",
        "\n",
        "    # Generate a final summary from the chunk summaries\n",
        "    response = openai.Completion.create(\n",
        "        model=\"text-davinci-003\", prompt=prompt, max_tokens=2056\n",
        "    )\n",
        "    final_summary = re.sub(\"\\s+\", \" \", response.choices[0].text.strip())\n",
        "\n",
        "    # Return all of the summaries as a dictionary\n",
        "    \n",
        "    return final_summary\n",
        "    \n",
        "\n",
        "def summarize_website(url):\n",
        "    # Get the text content from the website\n",
        "    print(\"website0\")\n",
        "    website_text = get_website_text(url)\n",
        "    print(\"website1\")\n",
        "\n",
        "\n",
        "    # Split the text content into chunks of characters\n",
        "    chunk_size = 2000\n",
        "    chunks = textwrap.wrap(website_text, chunk_size)\n",
        "    print(chunks)\n",
        "    summaries = list()\n",
        "\n",
        "    # For each chunk of characters, generate a summary\n",
        "    for chunk in chunks:\n",
        "        print(\"website2\")\n",
        "        prompt = PROMPT_STRING.replace(\"<<SUMMARY>>\", chunk)\n",
        "\n",
        "        print(f\"prompt0: {prompt}\")\n",
        "        # Generate summary using GPT-3\n",
        "        response = openai.Completion.create(\n",
        "            model=\"text-davinci-002\", prompt=prompt, max_tokens=256\n",
        "        )\n",
        "        print(\"website3\")\n",
        "\n",
        "        summary = re.sub(\"\\s+\", \" \", response.choices[0].text.strip())\n",
        "        summaries.append(summary)\n",
        "\n",
        "    # Join the chunk summaries into one string\n",
        "    chunk_summaries = \" \".join(summaries)\n",
        "    prompt = PROMPT_STRING.replace(\"<<SUMMARY>>\", chunk_summaries)\n",
        "    print(f\"prompt: {prompt}\")\n",
        "\n",
        "    # Generate a final summary from the chunk summaries\n",
        "    response = openai.Completion.create(\n",
        "        model=\"text-davinci-002\", prompt=prompt, max_tokens=2056\n",
        "    )\n",
        "    final_summary = re.sub(\"\\s+\", \" \", response.choices[0].text.strip())\n",
        "    print(\"website5\")\n",
        "\n",
        "    return final_summary\n",
        "\n",
        "from langdetect import detect\n",
        "def generate_convert(message):\n",
        "    # Detect the language of the input text\n",
        "    language = detect(message)\n",
        "    # Call the GPT-3 API to generate summary\n",
        "    prompt = f\"Please generate a summary in {language} for the following text:\\n{message}\"\n",
        "    response = openai.Completion.create(\n",
        "        engine='text-davinci-002',\n",
        "        prompt=prompt,\n",
        "        max_tokens=1024,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.3,\n",
        "    )\n",
        "    # Get the generated summary from the API response\n",
        "    summary = response.choices[0].text.strip()\n",
        "    return summary    \n",
        "def summarize_pdf(file_name):\n",
        "    # Extract text from PDF\n",
        "    text = textract.process(file_name).decode('utf-8')\n",
        "\n",
        "    # Summarize the extracted text using GPT-3\n",
        "    summary = openai.Completion.create(\n",
        "        model=\"text-davinci-002\",\n",
        "        prompt=f\"Please summarize the following text:\\n{text}\",\n",
        "        temperature=0,\n",
        "        # max_tokens=200\n",
        "    )\n",
        "\n",
        "    return summary.choices[0].text.strip()\n",
        "def summarize_text(text):\n",
        "    # Split the text into chunks of characters\n",
        "    chunk_size = 2000\n",
        "    chunks = textwrap.wrap(text, chunk_size)\n",
        "\n",
        "    summaries = list()\n",
        "\n",
        "    # For each chunk of characters, generate a summary\n",
        "    for chunk in chunks:\n",
        "        prompt = PROMPT_STRING.replace(\"<<SUMMARY>>\", chunk)\n",
        "\n",
        "        # Generate summary using GPT-3\n",
        "        response = openai.Completion.create(\n",
        "            model=\"text-davinci-002\", prompt=prompt, max_tokens=256\n",
        "        )\n",
        "        summary = re.sub(\"\\s+\", \" \", response.choices[0].text.strip())\n",
        "        summaries.append(summary)\n",
        "\n",
        "    # Join the chunk summaries into one string\n",
        "    chunk_summaries = \" \".join(summaries)\n",
        "    prompt = PROMPT_STRING.replace(\"<<SUMMARY>>\", chunk_summaries)\n",
        "\n",
        "    # Generate a final summary from the chunk summaries\n",
        "    response = openai.Completion.create(\n",
        "        model=\"text-davinci-002\", prompt=prompt, max_tokens=2056\n",
        "    )\n",
        "    final_summary = re.sub(\"\\s+\", \" \", response.choices[0].text.strip())\n",
        "\n",
        "    return final_summary\n",
        "\n",
        "def get_website_text(url):\n",
        "    # Make a GET request to the website URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Parse the HTML content using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extract the text content from the HTML using the 'get_text' method\n",
        "    website_text = soup.get_text()\n",
        "\n",
        "    # Remove any extra whitespace and newlines\n",
        "    website_text = re.sub(r'\\s+', ' ', website_text).strip()\n",
        "    print(website_text)\n",
        "    return website_text\n",
        "\n",
        "# Create a function to determine the type of the link\n",
        "\n",
        "# Create a function to handle incoming messages\n",
        "\n",
        "@bot.message_handler(commands=['Greet'])\n",
        "def greet(message):\n",
        "    bot.reply_to(message, \"Hey! How's it going?\")\n",
        "\n",
        "@bot.message_handler(commands=['hello'])\n",
        "def hello(message):\n",
        "    bot.send_message(message.chat.id, \"Hello!\")\n",
        "\n",
        "@bot.message_handler(commands=['start'])\n",
        "def ask_content_type(message):\n",
        "    bot.send_message(chat_id=message.chat.id, text=\"What type of content do you want to do? Please choose one of the following options:\\n\\n1. Summarize YouTube video\\n2. Summarize Website link\\n3. Summarize Document\\n4. Summarize text\\n5. Convert to English\")\n",
        "\n",
        "    bot.register_next_step_handler(message, process_content_type)\n",
        "\n",
        "def process_content_type(message):\n",
        "    content_type = message.text.lower()\n",
        "\n",
        "    if content_type == '1':\n",
        "        bot.send_message(chat_id=message.chat.id, text=\"Please enter the YouTube video URL you want to summarize\")\n",
        "        bot.register_next_step_handler(message, summarize_youtube_video)\n",
        "    elif content_type == '2':\n",
        "        bot.send_message(chat_id=message.chat.id, text=\"Please enter the website URL you want to summarize\")\n",
        "        bot.register_next_step_handler(message, summarize_website_link)\n",
        "    elif content_type == '3':\n",
        "        bot.send_message(chat_id=message.chat.id, text=\"Please upload the document you want to summarize\")\n",
        "        bot.register_next_step_handler(message, summarize_document)\n",
        "    elif content_type == '5':\n",
        "        bot.send_message(chat_id=message.chat.id, text=\"Please upload the text you want to convert\")\n",
        "        bot.register_next_step_handler(message, converter)\n",
        "    elif content_type == '4':\n",
        "        bot.send_message(chat_id=message.chat.id, text=\"Please upload the text you want to summarize\")\n",
        "        bot.register_next_step_handler(message, converter)    \n",
        "    else:\n",
        "        bot.send_message(chat_id=message.chat.id, text=\"Sorry, I didn't understand that. Please choose one of the following options:\\n\\n1. YouTube video\\n2. Website link\\n3. Document\\n4. Convert to English\")\n",
        "        bot.register_next_step_handler(message, process_content_type)\n",
        "def summarizer_text(message):\n",
        "    print(\"d\")\n",
        "    text=message.text\n",
        "    summary=summarize_text(text)\n",
        "    sent=sentiment(summary)\n",
        "    bot.send_message(chat_id=message.chat.id, text=summary)\n",
        "    bot.send_message(chat_id=message.chat.id, text=sent)\n",
        "def converter(message):\n",
        "    print(\"c\")\n",
        "    conv=message.text\n",
        "    summary= generate_convert(conv)\n",
        "    sent=sentiment(summary)\n",
        "    bot.send_message(chat_id=message.chat.id, text=summary)\n",
        "    bot.send_message(chat_id=message.chat.id, text=sent)\n",
        "def summarize_youtube_video(message):\n",
        "    print(\"yt\")\n",
        "    video_url = message.text\n",
        "    summary = summarize_video(video_url)\n",
        "    sent=sentiment(summary)\n",
        "    bot.send_message(chat_id=message.chat.id, text=summary)\n",
        "    bot.send_message(chat_id=message.chat.id, text=sent)\n",
        "def summarize_website_link(message):\n",
        "    print(\"website\")\n",
        "    website_url = message.text\n",
        "    summary = summarize_website(website_url)\n",
        "    sent=sentiment(summary)\n",
        "    bot.send_message(chat_id=message.chat.id, text=summary)\n",
        "    bot.send_message(chat_id=message.chat.id, text=sent)\n",
        "@bot.message_handler(content_types=['document'])\n",
        "def summarize_document(message):\n",
        "    # handle uploaded document\n",
        "        file_id = message.document.file_id\n",
        "        file_name = message.document.file_name\n",
        "\n",
        "        bot = telebot.TeleBot(\"5632868573:AAHVK_LEzgFKNgy9SHUPC2M_9yX-B1l8YDs\")\n",
        "        file_info = bot.get_file(file_id)\n",
        "        file_path = file_info.file_path\n",
        "\n",
        "        # Download the file to the server\n",
        "        downloaded_file = bot.download_file(file_path)\n",
        "        with open(file_name, 'wb') as f:\n",
        "            f.write(downloaded_file)\n",
        "        \n",
        "        pdf_path = file_name\n",
        "        \n",
        "        textt = summarize_pdf(pdf_path)\n",
        "        bot.send_message(chat_id=message.chat.id, text=textt)\n",
        "        sent=sentiment(textt)\n",
        "        bot.send_message(chat_id=message.chat.id, text=sent)\n",
        "bot.polling()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZVwWvt2wbMo",
        "outputId": "050e7a78-419e-4224-ed33-9cea24a37ce0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yt\n"
          ]
        }
      ]
    }
  ]
}